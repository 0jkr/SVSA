#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SVSA Monolithic Application
--------------------------

This single-file application implements a basic but extensible vulnerability scanner and analyzer.
It is intentionally written as one file (monolith) and contains many components usually split into
multiple modules: CLI, HTTP API (FastAPI), scanner engine, analyzer (ML), storage, reporting, and test helpers.

Safety & Consent:
- The program will refuse to run scans unless explicit consent is provided via:
    - CLI flag: --confirm-consent
    - Environment variable: SVSA_CONFIRM=1
    - When running via API, the server process must have SVSA_CONFIRM set in its environment.
- The default scanning profile is "safe", which avoids destructive techniques.

Usage (examples):
    python -m svsa_monolith scan --target http://example.com --profile safe --confirm-consent
    export SVSA_CONFIRM=1
    python -m svsa_monolith server

Note: This file intentionally contains verbose comments, templates, and example logic to make it self-contained
and educational. It is not optimized for production performance or security. Consider splitting into modules,
adding authentication, and thorough testing before production use.

Author: Autogenerated by ChatGPT for the SVSA Monolithic Edition
"""

# --------------------------
# Standard library imports
# --------------------------
import argparse
import base64
import contextlib
import datetime
import hashlib
import http
import http.client
import json
import logging
import os
import re
import secrets
import signal
import socket
import sqlite3
import ssl
import string
import subprocess
import sys
import threading
import time
import traceback
import typing
from dataclasses import dataclass, field, asdict
from functools import wraps
from typing import Any, Dict, List, Optional, Tuple

# --------------------------
# Third-party imports
# --------------------------
try:
    import httpx
except Exception:
    httpx = None

try:
    from fastapi import FastAPI, HTTPException
    from pydantic import BaseModel
    import uvicorn
except Exception:
    FastAPI = None
    HTTPException = Exception
    BaseModel = object
    uvicorn = None

# ML libs
try:
    import numpy as np
    import pandas as pd
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import classification_report
    from sklearn.preprocessing import StandardScaler
    import joblib
except Exception:
    # Provide placeholders if ML libs are not installed; this avoids import-time failures.
    np = None
    pd = None
    RandomForestClassifier = None
    train_test_split = None
    classification_report = None
    StandardScaler = None
    joblib = None

# --------------------------
# Logging configuration
# --------------------------
LOG = logging.getLogger("svsa_monolith")
LOG.setLevel(logging.INFO)
_handler = logging.StreamHandler(sys.stdout)
_handler.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(message)s"))
LOG.addHandler(_handler)

# --------------------------
# Constants and Config
# --------------------------
VERSION = "0.1.0-monolith"
DB_FILE = os.environ.get("SVSA_DB", "svsa_monolith.db")
MODEL_FILE = os.environ.get("SVSA_MODEL", "svsa_model.joblib")
CONFIRM_ENV = "SVSA_CONFIRM"
DEFAULT_TIMEOUT = 8.0  # seconds for network operations
TCP_CONNECT_TIMEOUT = 3.0
MAX_CONCURRENT_PROBES = 8
USER_AGENT = f"SVSA/Monolith/{VERSION}"

# Profiles
PROFILES = {
    "safe": {"tcp_probe": False, "nmap": False, "http_active": False},
    "quick": {"tcp_probe": True, "nmap": True, "http_active": False},
    "full": {"tcp_probe": True, "nmap": True, "http_active": True},
}

# Risk thresholds (probabilities from analyzer)
RISK_THRESHOLDS = {
    "low": 0.15,
    "medium": 0.4,
    "high": 0.75,
    # critical above 0.95
}

# --------------------------
# Utilities
# --------------------------

def require_consent_cli(provided: bool) -> None:
    """
    Enforce consent for CLI scanning runs.

    The user must either pass --confirm-consent or set SVSA_CONFIRM=1 in the environment.
    """
    if provided:
        return
    env_val = os.environ.get(CONFIRM_ENV, "")
    if env_val in ("1", "true", "True", "yes", "YES"):
        return
    LOG.error("Consent not provided. To run scans you must pass --confirm-consent or set %s=1", CONFIRM_ENV)
    raise PermissionError("Consent required to run scans. Provide --confirm-consent or set SVSA_CONFIRM=1")


def require_consent_server() -> None:
    """
    Server must be started with consent environment variable set to allow scanning via API calls.
    This prevents accidental exposure of scan capability through an unauthenticated HTTP API.
    """
    env_val = os.environ.get(CONFIRM_ENV, "")
    if env_val not in ("1", "true", "True", "yes", "YES"):
        LOG.error("Server started without %s set to allow scans via the API. Set %s=1 to enable.", CONFIRM_ENV, CONFIRM_ENV)
        # Not raising to allow server to run, but API scan endpoints will block unless env var set.
    else:
        LOG.info("Server environment contains %s=1; API scans are permitted.", CONFIRM_ENV)


def normalize_target(target: str) -> str:
    """
    Normalize target input to a canonical form used internally.
    Accepts: IP, host, http://host[:port]/path, https://...
    Returns a canonical URL-like string or host string.
    """
    t = target.strip()
    if t.startswith("http://") or t.startswith("https://"):
        return t
    # If it contains scheme-less but includes path, leave as-is
    if "/" in t:
        return "http://" + t
    # else treat as host
    return t

def now_iso() -> str:
    return datetime.datetime.utcnow().replace(microsecond=0).isoformat() + "Z"

def generate_id(prefix: str = "scan") -> str:
    rnd = secrets.token_hex(12)
    return f"{prefix}_{int(time.time())}_{rnd}"

def safe_repr(obj: Any, max_len: int = 512) -> str:
    s = repr(obj)
    if len(s) > max_len:
        return s[:max_len] + "...(truncated)"
    return s

# --------------------------
# Database (SQLite) layer
# --------------------------

class Storage:
    """
    Lightweight SQLite-based storage for scans, findings, analyses, and labels.

    Schema (simple):
      - scans(id TEXT PRIMARY KEY, target TEXT, profile TEXT, created_at TEXT, raw_json TEXT)
      - findings(id TEXT PRIMARY KEY, scan_id TEXT, source TEXT, target TEXT, data_json TEXT)
      - analyses(id TEXT PRIMARY KEY, scan_id TEXT, finding_id TEXT, label TEXT, score REAL, meta_json TEXT)
      - labels(id TEXT PRIMARY KEY, scan_id TEXT, finding_id TEXT, label INTEGER, created_at TEXT)
      - audit(id TEXT PRIMARY KEY, ts TEXT, action TEXT, payload TEXT)
    """

    def __init__(self, db_file: str = DB_FILE):
        self.db_file = db_file
        self._conn = sqlite3.connect(self.db_file, check_same_thread=False)
        self._conn.row_factory = sqlite3.Row
        self._init_db()

    def _init_db(self) -> None:
        c = self._conn.cursor()
        c.execute(
            """
            CREATE TABLE IF NOT EXISTS scans (
              id TEXT PRIMARY KEY,
              target TEXT,
              profile TEXT,
              created_at TEXT,
              raw_json TEXT
            )
            """
        )
        c.execute(
            """
            CREATE TABLE IF NOT EXISTS findings (
              id TEXT PRIMARY KEY,
              scan_id TEXT,
              source TEXT,
              target TEXT,
              data_json TEXT
            )
            """
        )
        c.execute(
            """
            CREATE TABLE IF NOT EXISTS analyses (
              id TEXT PRIMARY KEY,
              scan_id TEXT,
              finding_id TEXT,
              label TEXT,
              score REAL,
              meta_json TEXT
            )
            """
        )
        c.execute(
            """
            CREATE TABLE IF NOT EXISTS labels (
              id TEXT PRIMARY KEY,
              scan_id TEXT,
              finding_id TEXT,
              label INTEGER,
              created_at TEXT
            )
            """
        )
        c.execute(
            """
            CREATE TABLE IF NOT EXISTS audit (
              id TEXT PRIMARY KEY,
              ts TEXT,
              action TEXT,
              payload TEXT
            )
            """
        )
        self._conn.commit()

    def save_scan(self, target: str, profile: str, raw_result: Dict[str, Any]) -> str:
        scan_id = generate_id("scan")
        c = self._conn.cursor()
        c.execute(
            "INSERT INTO scans(id, target, profile, created_at, raw_json) VALUES (?, ?, ?, ?, ?)",
            (scan_id, target, profile, now_iso(), json.dumps(raw_result)),
        )
        self._conn.commit()
        self._audit("save_scan", {"scan_id": scan_id, "target": target, "profile": profile})
        return scan_id

    def save_finding(self, scan_id: str, source: str, target: str, data: Dict[str, Any]) -> str:
        fid = generate_id("finding")
        c = self._conn.cursor()
        c.execute(
            "INSERT INTO findings(id, scan_id, source, target, data_json) VALUES (?, ?, ?, ?, ?)",
            (fid, scan_id, source, target, json.dumps(data)),
        )
        self._conn.commit()
        self._audit("save_finding", {"finding_id": fid, "scan_id": scan_id, "source": source})
        return fid

    def list_scans(self, limit: int = 50) -> List[Dict[str, Any]]:
        c = self._conn.cursor()
        c.execute("SELECT id, target, profile, created_at FROM scans ORDER BY created_at DESC LIMIT ?", (limit,))
        rows = c.fetchall()
        out = []
        for r in rows:
            out.append({"id": r["id"], "target": r["target"], "profile": r["profile"], "created_at": r["created_at"]})
        return out

    def get_scan(self, scan_id: str) -> Optional[Dict[str, Any]]:
        c = self._conn.cursor()
        c.execute("SELECT * FROM scans WHERE id = ?", (scan_id,))
        r = c.fetchone()
        if not r:
            return None
        raw = json.loads(r["raw_json"])
        # load findings
        c.execute("SELECT * FROM findings WHERE scan_id = ?", (scan_id,))
        findings = []
        for f in c.fetchall():
            findings.append({"id": f["id"], "source": f["source"], "target": f["target"], "data": json.loads(f["data_json"])})
        # analyses
        c.execute("SELECT * FROM analyses WHERE scan_id = ?", (scan_id,))
        analyses = []
        for a in c.fetchall():
            analyses.append({"id": a["id"], "finding_id": a["finding_id"], "label": a["label"], "score": a["score"], "meta": json.loads(a["meta_json"] or "{}")})
        return {"id": r["id"], "target": r["target"], "profile": r["profile"], "created_at": r["created_at"], "raw": raw, "findings": findings, "analyses": analyses}

    def save_analysis(self, scan_id: str, finding_id: str, label: str, score: float, meta: Dict[str, Any]) -> str:
        aid = generate_id("analysis")
        c = self._conn.cursor()
        c.execute(
            "INSERT INTO analyses(id, scan_id, finding_id, label, score, meta_json) VALUES (?, ?, ?, ?, ?, ?)",
            (aid, scan_id, finding_id, label, score, json.dumps(meta)),
        )
        self._conn.commit()
        self._audit("save_analysis", {"analysis_id": aid, "scan_id": scan_id, "finding_id": finding_id, "label": label})
        return aid

    def save_label(self, scan_id: str, finding_id: str, label: bool) -> str:
        lid = generate_id("label")
        c = self._conn.cursor()
        c.execute(
            "INSERT INTO labels(id, scan_id, finding_id, label, created_at) VALUES (?, ?, ?, ?, ?)",
            (lid, scan_id, finding_id, 1 if label else 0, now_iso()),
        )
        self._conn.commit()
        self._audit("save_label", {"label_id": lid, "scan_id": scan_id, "finding_id": finding_id, "label": label})
        return lid

    def get_labels_for_training(self) -> List[Tuple[str, str, int]]:
        c = self._conn.cursor()
        c.execute("SELECT scan_id, finding_id, label FROM labels")
        return [(r["scan_id"], r["finding_id"], r["label"]) for r in c.fetchall()]

    def _audit(self, action: str, payload: Dict[str, Any]) -> None:
        aid = generate_id("audit")
        c = self._conn.cursor()
        c.execute("INSERT INTO audit(id, ts, action, payload) VALUES (?, ?, ?, ?)", (aid, now_iso(), action, json.dumps(payload)))
        self._conn.commit()

# --------------------------
# Scanning Engine
# --------------------------

@dataclass
class Finding:
    id: str
    scan_id: str
    source: str
    target: str
    name: str
    description: str
    evidence: Dict[str, Any]
    confidence: float = 0.0
    created_at: str = field(default_factory=now_iso)

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)

class Scanner:
    """
    Scanner orchestrator. For monolith, we include simple built-in probes:
      - banner_grab for given host:port
      - tcp_connect for ports
      - nmap wrapper (if nmap present and allowed by profile)
      - http checks for common OWASP-like fingerprints (no exploitation)
    """

    def __init__(self, timeout: float = DEFAULT_TIMEOUT, max_workers: int = MAX_CONCURRENT_PROBES):
        self.timeout = timeout
        self.max_workers = max_workers
        # threadpool for simple concurrency
        self._pool = threading.BoundedSemaphore(max_workers)

    def scan_target(self, target: str, profile: str = "safe", safe: bool = True) -> Dict[str, Any]:
        """
        Perform a scan of the provided target and return a dictionary with findings and metadata.

        Parameters:
            target: host, ip, or URL
            profile: one of PROFILES
            safe: if True, restrict active probing; otherwise follow profile settings

        Returns:
            dict with keys: id, target, profile, created_at, findings (list)
        """
        final_profile = profile if profile in PROFILES else "safe"
        LOG.info("Starting scan: target=%s profile=%s safe=%s", target, final_profile, safe)
        scan_id = generate_id("scan")
        normalized = normalize_target(target)
        start_ts = now_iso()
        findings: List[Dict[str, Any]] = []

        # Basic resolution and host parsing
        host, port, is_http = self._extract_host_port(normalized)
        LOG.debug("Parsed host=%s port=%s is_http=%s", host, port, is_http)

        # perform DNS resolution attempt
        resolved_ips = []
        try:
            resolved_ips = list({ai[4][0] for ai in socket.getaddrinfo(host, None)})
            LOG.info("Resolved %s -> %s", host, resolved_ips)
        except Exception as e:
            LOG.warning("DNS resolution failed for %s: %s", host, safe_repr(e))

        # Always try banner grabbing for common ports if allowed
        if PROFILES[final_profile].get("tcp_probe", False) and not (safe and final_profile == "safe"):
            # probe a set of common ports for banners
            common_ports = [80, 443, 22, 21, 25, 53, 3306, 5432, 8080, 8443]
            # If user supplied a port, ensure it is included
            if port and isinstance(port, int) and port not in common_ports:
                common_ports.insert(0, port)
            LOG.info("TCP banner probe ports: %s", common_ports)
            for p in common_ports:
                # Respect rate limiting: try to acquire a slot
                try:
                    self._pool.acquire()
                    try:
                        b = self._tcp_banner(host, p, timeout=TCP_CONNECT_TIMEOUT)
                        if b:
                            f = Finding(
                                id=generate_id("finding"),
                                scan_id=scan_id,
                                source="tcp_banner",
                                target=f"{host}:{p}",
                                name="Network banner",
                                description=f"Banner grabbed on {host}:{p}",
                                evidence={"banner": b},
                                confidence=0.2,
                            )
                            findings.append(f.to_dict())
                    finally:
                        self._pool.release()
                except Exception as e:
                    LOG.debug("Error probing %s:%s - %s", host, p, safe_repr(e))

        # Nmap wrapper (if allowed by profile and nmap binary present)
        if PROFILES[final_profile].get("nmap", False) and not (safe and final_profile == "safe"):
            if self._nmap_available():
                LOG.info("Running nmap scan for target=%s", host)
                try:
                    nmap_out = self._run_nmap(host)
                    # nmap_out expected as structured dict
                    if nmap_out:
                        # convert nmap output into findings
                        for port_obj in nmap_out.get("ports", []):
                            f = Finding(
                                id=generate_id("finding"),
                                scan_id=scan_id,
                                source="nmap",
                                target=f"{host}:{port_obj.get('port')}",
                                name=f"Service: {port_obj.get('service')}/{port_obj.get('product', '')}",
                                description="Service detected by nmap",
                                evidence=port_obj,
                                confidence=0.3,
                            )
                            findings.append(f.to_dict())
                except Exception as e:
                    LOG.warning("nmap scan failed: %s", safe_repr(e))
            else:
                LOG.info("nmap not available; skipping nmap stage")

        # HTTP checks (passive, fingerprinting)
        if is_http and (PROFILES[final_profile].get("http_active", False) and not (safe and final_profile == "safe")):
            LOG.info("Performing HTTP checks against %s", normalized)
            try:
                http_findings = self._http_checks(normalized)
                for hf in http_findings:
                    f = Finding(
                        id=generate_id("finding"),
                        scan_id=scan_id,
                        source="http_checks",
                        target=normalized,
                        name=hf.get("name", "HTTP issue"),
                        description=hf.get("desc", ""),
                        evidence=hf.get("evidence", {}),
                        confidence=hf.get("confidence", 0.2),
                    )
                    findings.append(f.to_dict())
            except Exception as e:
                LOG.warning("HTTP checks failed for %s: %s", normalized, safe_repr(e))
        else:
            # If http_active disabled or safe profile, we still do a passive header grab when possible
            if is_http:
                try:
                    headers = self._http_head(normalized)
                    if headers:
                        f = Finding(
                            id=generate_id("finding"),
                            scan_id=scan_id,
                            source="http_headers",
                            target=normalized,
                            name="HTTP headers",
                            description="Collected HTTP response headers (passive)",
                            evidence={"headers": headers},
                            confidence=0.15,
                        )
                        findings.append(f.to_dict())
                except Exception as e:
                    LOG.debug("Passive HTTP header grab failed: %s", safe_repr(e))

        # TLS cert metadata extraction for https targets (passive)
        if normalized.startswith("https://") or port == 443:
            try:
                cert = self._get_tls_cert(host, port or 443)
                if cert:
                    f = Finding(
                        id=generate_id("finding"),
                        scan_id=scan_id,
                        source="tls_cert",
                        target=host,
                        name="TLS Certificate",
                        description="Extracted TLS certificate metadata",
                        evidence=cert,
                        confidence=0.1,
                    )
                    findings.append(f.to_dict())
            except Exception as e:
                LOG.debug("TLS cert extraction failed: %s", safe_repr(e))

        # Basic CVE/fingerprint matching (heuristic)
        try:
            matched = self._fingerprint_match(findings)
            for m in matched:
                # matched items are dicts
                f = Finding(
                    id=generate_id("finding"),
                    scan_id=scan_id,
                    source="fingerprint",
                    target=m.get("target", target),
                    name=m.get("name", "Fingerprint match"),
                    description=m.get("desc", ""),
                    evidence=m.get("evidence", {}),
                    confidence=m.get("confidence", 0.5),
                )
                findings.append(f.to_dict())
        except Exception as e:
            LOG.debug("Fingerprint matching failed: %s", safe_repr(e))

        # Consolidate scan result
        result = {
            "id": scan_id,
            "target": target,
            "normalized": normalized,
            "profile": final_profile,
            "created_at": start_ts,
            "findings": [f for f in findings],
        }
        LOG.info("Scan complete: %d findings", len(findings))
        return result

    # --------------------------
    # Low-level probes
    # --------------------------
    def _tcp_banner(self, host: str, port: int, timeout: float = 3.0) -> Optional[str]:
        """
        Attempt to connect and read a small banner. Non-intrusive; only connects and reads.
        """
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        s.settimeout(timeout)
        try:
            s.connect((host, port))
            # send nothing; try to read
            try:
                data = s.recv(1024)
                if data:
                    try:
                        return data.decode("utf-8", errors="ignore")
                    except Exception:
                        return base64.b64encode(data).decode("ascii")
                # If no data received, try a simple probe (e.g., HTTP GET for port 80)
                if port in (80, 8080, 8000, 8888):
                    # perform a tiny HTTP GET politely
                    req = b"GET / HTTP/1.0\r\nUser-Agent: %s\r\n\r\n" % USER_AGENT.encode("utf-8")
                    s.sendall(req)
                    data2 = s.recv(2048)
                    return data2.decode("utf-8", errors="ignore")
            except Exception:
                return None
        except Exception as e:
            LOG.debug("tcp_banner connect failed for %s:%s - %s", host, port, safe_repr(e))
            return None
        finally:
            with contextlib.suppress(Exception):
                s.close()

    def _nmap_available(self) -> bool:
        """
        Check if the nmap binary exists in PATH.
        """
        try:
            res = subprocess.run(["nmap", "--version"], capture_output=True, text=True, timeout=3)
            if res.returncode == 0:
                LOG.debug("nmap available: %s", res.stdout.splitlines()[0] if res.stdout else res.stderr.splitlines()[0])
                return True
        except Exception:
            pass
        return False

    def _run_nmap(self, host: str) -> Dict[str, Any]:
        """
        Run nmap with safe flags and parse output with grepable or XML output.
        For simplicity we run a service/version scan and parse the plain text.
        NOTE: Requires nmap installed on the host.
        """
        # Safe nmap flags: -sV (service/version), -Pn (no ping), -p-? (limited)
        args = ["nmap", "-sV", "-Pn", host]
        try:
            res = subprocess.run(args, capture_output=True, text=True, timeout=60)
        except Exception as e:
            LOG.warning("nmap execution failed: %s", safe_repr(e))
            return {}
        out = res.stdout or res.stderr
        ports = []
        # Very simple parser: find lines like "22/tcp open ssh OpenSSH 7.4"
        for line in out.splitlines():
            m = re.match(r"^(\d+)\/tcp\s+(\w+)\s+([\w\-]+)\s*(.*)$", line)
            if m:
                portnum = int(m.group(1))
                state = m.group(2)
                service = m.group(3)
                rest = m.group(4).strip()
                ports.append({"port": portnum, "state": state, "service": service, "product": rest})
        return {"raw": out, "ports": ports}

    def _http_head(self, url: str) -> Dict[str, Any]:
        """
        Simple HEAD request to collect headers; falls back to GET on some servers if HEAD blocked.
        Uses httpx if available, otherwise uses http.client.
        """
        headers = {}
        if httpx:
            try:
                with httpx.Client(timeout=self.timeout, headers={"User-Agent": USER_AGENT}) as c:
                    r = c.head(url, follow_redirects=True)
                    headers = dict(r.headers)
                    return headers
            except Exception:
                try:
                    with httpx.Client(timeout=self.timeout, headers={"User-Agent": USER_AGENT}) as c:
                        r = c.get(url, follow_redirects=True)
                        return dict(r.headers)
                except Exception as e:
                    LOG.debug("http_head error (httpx): %s", safe_repr(e))
                    return {}
        # fallback using urllib/http.client
        try:
            from urllib.parse import urlparse
            p = urlparse(url)
            conn = http.client.HTTPSConnection(p.netloc, timeout=self.timeout) if p.scheme == "https" else http.client.HTTPConnection(p.netloc, timeout=self.timeout)
            path = p.path or "/"
            conn.request("HEAD", path, headers={"User-Agent": USER_AGENT})
            resp = conn.getresponse()
            headers = dict(resp.getheaders())
            conn.close()
            return headers
        except Exception as e:
            LOG.debug("http_head fallback failed: %s", safe_repr(e))
            return {}

    def _http_checks(self, url: str) -> List[Dict[str, Any]]:
        """
        Non-destructive HTTP checks for fingerprint patterns that may indicate common issues (OWASP-like).
        Examples (non-exhaustive):
          - missing security headers (X-Frame-Options, X-XSS-Protection, Content-Security-Policy)
          - server header exposing product/version
          - suspicious query parameter reflection on page indicating possible XSS vector (passive detection)
          - SQL error strings in response body (passive)
        This function does NOT attempt to inject payloads or exploit vulnerabilities.
        """
        results = []
        headers = self._http_head(url)
        # security header checks
        missing = []
        for h in ("Content-Security-Policy", "X-Frame-Options", "X-XSS-Protection", "Strict-Transport-Security", "Referrer-Policy"):
            if h not in headers:
                missing.append(h)
        if missing:
            results.append({"name": "Missing security headers", "desc": f"Missing headers: {', '.join(missing)}", "evidence": {"missing": missing}, "confidence": 0.2})

        # server header leakage
        server = headers.get("Server") or headers.get("server")
        if server:
            results.append({"name": "Server header", "desc": f"Server header exposes: {server}", "evidence": {"server": server}, "confidence": 0.10})

        # passive reflection detection: fetch body and check for reflected params or common error messages
        body = ""
        if httpx:
            try:
                with httpx.Client(timeout=self.timeout, headers={"User-Agent": USER_AGENT}) as c:
                    r = c.get(url, follow_redirects=True)
                    body = r.text[:10000]
            except Exception as e:
                LOG.debug("http_checks httpx get failed: %s", safe_repr(e))
                body = ""
        else:
            # fallback minimal fetch (not robust)
            try:
                import urllib.request as ur
                req = ur.Request(url, headers={"User-Agent": USER_AGENT})
                with ur.urlopen(req, timeout=self.timeout) as resp:
                    body = resp.read(10000).decode("utf-8", errors="ignore")
            except Exception as e:
                LOG.debug("http_checks urllib get failed: %s", safe_repr(e))
                body = ""

        # look for SQL error patterns (passive detection only)
        sql_errors = ["you have an error in your sql syntax", "mysql_fetch", "syntax error", "native client", "odbc", "sql syntax"]
        for token in sql_errors:
            if token.lower() in body.lower():
                results.append({"name": "Potential SQL error disclosure", "desc": "Response body contains SQL error patterns (passive)", "evidence": {"snippet": token}, "confidence": 0.5})

        # reflection detection simple heuristic: check if common parameter names appear reflected verbatim
        # extract query parameters if present
        if "?" in url:
            try:
                from urllib.parse import parse_qs, urlparse, unquote
                q = urlparse(url).query
                params = parse_qs(q)
                for k, vs in params.items():
                    for v in vs:
                        if v and v in body:
                            results.append({"name": "Possible reflected input", "desc": f"Parameter {k} appears reflected in page body", "evidence": {"param": k, "value": v}, "confidence": 0.25})
            except Exception:
                pass

        # fingerprint framework: detect known CMS product strings in body or headers
        cms_signatures = [
            ("wp-content", "WordPress"),
            ("Joomla!", "Joomla"),
            ("Drupal", "Drupal"),
            ("Shopify", "Shopify"),
            ("Magento", "Magento"),
        ]
        for sig, name in cms_signatures:
            if (sig.lower() in body.lower()) or (sig in (headers.get("Server", "") + headers.get("X-Powered-By", ""))):
                results.append({"name": f"CMS detected: {name}", "desc": f"Fingerprint suggests CMS {name}", "evidence": {"signature": sig}, "confidence": 0.35})

        return results

    def _get_tls_cert(self, host: str, port: int = 443) -> Dict[str, Any]:
        """
        Extract TLS certificate metadata (subject, issuer, validity dates, SANs).
        Uses the ssl module to retrieve certificate without verifying it.
        """
        ctx = ssl.create_default_context()
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        with socket.create_connection((host, port), timeout=self.timeout) as sock:
            with ctx.wrap_socket(sock, server_hostname=host) as ssock:
                cert = ssock.getpeercert()
                # convert to dict with key metadata
                subject = dict(x[0] for x in cert.get("subject", ()))
                issuer = dict(x[0] for x in cert.get("issuer", ()))
                san = cert.get("subjectAltName", ())
                san_list = [v for (k, v) in san]
                return {"subject": subject, "issuer": issuer, "notBefore": cert.get("notBefore"), "notAfter": cert.get("notAfter"), "san": san_list}

    def _fingerprint_match(self, findings: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Basic heuristic CVE/fingerprint matching. In this monolith we include a small static database
        of example fingerprints. A production system would sync with NVD/OSV and maintain a larger dataset.

        Returns a list of matched candidate dicts.
        """
        matches = []
        # small example fingerprint DB
        fingerprints = [
            {"name": "OpenSSH old version", "indicator": "OpenSSH 6.", "desc": "Old OpenSSH version (6.x) may contain known issues", "confidence": 0.6},
            {"name": "Apache httpd version", "indicator": "Apache/2.2", "desc": "Apache 2.2 is EOL and may contain vulnerabilities", "confidence": 0.6},
            {"name": "WordPress version disclosure", "indicator": "wp-content", "desc": "WordPress detected; check plugin versions", "confidence": 0.4},
        ]
        # iterate over finding evidence to locate indicators
        for f in findings:
            data = f.get("evidence", {})
            # check banner, headers, product strings
            banner = ""
            if isinstance(data, dict):
                banner = " ".join([str(v) for v in data.values() if isinstance(v, (str, int))])
            else:
                banner = str(data)
            for fp in fingerprints:
                if fp["indicator"].lower() in banner.lower():
                    matches.append({"target": f.get("target"), "name": fp["name"], "desc": fp["desc"], "evidence": {"banner": banner}, "confidence": fp["confidence"]})
        return matches

    def _extract_host_port(self, normalized: str) -> Tuple[str, Optional[int], bool]:
        """
        Return host, port (int or None), is_http boolean.
        """
        # naive parsing
        if normalized.startswith("http://") or normalized.startswith("https://"):
            from urllib.parse import urlparse
            p = urlparse(normalized)
            host = p.hostname or p.path
            port = p.port
            is_http = p.scheme in ("http", "https")
            return host, port, is_http
        # if contains colon and digits at end, treat as host:port
        m = re.match(r"^(.+):(\d+)$", normalized)
        if m:
            return m.group(1), int(m.group(2)), False
        return normalized, None, False

# --------------------------
# Analyzer (ML and heuristics)
# --------------------------

class Analyzer:
    """
    Analyzer contains featurization, a simple ML model manager, and convenience functions to
    produce an analysis for findings.

    The model is a RandomForest trained on synthetic features in this monolith as a placeholder.
    Real-world usage requires real labeled data and evaluation.
    """

    def __init__(self, model_file: str = MODEL_FILE):
        self.model_file = model_file
        self.model = None
        self.scaler = None
        self._load_model_if_exists()

    def _load_model_if_exists(self):
        if joblib and os.path.exists(self.model_file):
            try:
                LOG.info("Loading model from %s", self.model_file)
                obj = joblib.load(self.model_file)
                # Expecting dict with 'model' and 'scaler'
                if isinstance(obj, dict):
                    self.model = obj.get("model")
                    self.scaler = obj.get("scaler")
                else:
                    self.model = obj
                LOG.info("Model loaded")
            except Exception as e:
                LOG.warning("Failed to load model: %s", safe_repr(e))

    def featurize(self, finding: Dict[str, Any]) -> List[float]:
        """
        Convert a finding dict into a numeric feature vector.
        Features (example):
          - source categorical encoded into integers
          - length of evidence string
          - presence of certain keywords: "Apache", "OpenSSH", "wp-content", "SQL error"
          - confidence reported by scanner
        """
        src = finding.get("source", "")
        src_map = {"nmap": 1, "http_checks": 2, "http_headers": 3, "tcp_banner": 4, "tls_cert": 5, "fingerprint": 6}
        s_val = src_map.get(src, 0)
        evidence = finding.get("evidence", "")
        if isinstance(evidence, dict):
            evidence_str = " ".join([str(v) for v in evidence.values()])
        else:
            evidence_str = str(evidence)
        length = len(evidence_str)
        keywords = ["apache", "openssh", "wp-content", "drupal", "mysql", "error"]
        kw_features = [1.0 if kw in evidence_str.lower() else 0.0 for kw in keywords]
        scanner_conf = float(finding.get("confidence", 0.0))
        # example TLS features
        tls_ok = 0.0
        if finding.get("source") == "tls_cert" and isinstance(finding.get("evidence"), dict):
            not_after = finding["evidence"].get("notAfter")
            if not_after:
                try:
                    # not robust parsing; many formats exist
                    if "GMT" in not_after:
                        tls_ok = 1.0
                except Exception:
                    tls_ok = 0.0
        # Compose vector
        vec = [float(s_val), float(length), scanner_conf, tls_ok] + kw_features
        # pad/truncate to fixed size if necessary
        # final vector length here: 1 + 1 + 1 + 1 + len(keywords) = 1+1+1+1+6 = 10
        return vec

    def _vector_names(self) -> List[str]:
        return ["src_code", "evidence_len", "scanner_conf", "tls_flag", "kw_apache", "kw_openssh", "kw_wp", "kw_drupal", "kw_mysql", "kw_error"]

    def predict(self, findings: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Predict label probabilities for a list of findings.
        Returns a list of dicts: finding_id, probability, risk_label, explanation
        """
        results = []
        for f in findings:
            vec = self.featurize(f)
            prob = 0.0
            explanation = {"featurized": vec}
            if self.model and hasattr(self.model, "predict_proba"):
                try:
                    X = np.array(vec).reshape(1, -1)
                    if self.scaler:
                        X = self.scaler.transform(X)
                    p = self.model.predict_proba(X)[0]
                    # assume class 1 is "vulnerable"
                    # mapping depends on training
                    if len(p) == 2:
                        prob = float(p[1])
                    else:
                        prob = float(max(p))
                    explanation["model"] = "RandomForest"
                except Exception as e:
                    LOG.warning("Model prediction error: %s", safe_repr(e))
                    prob = 0.0
            else:
                # fallback heuristics: simple rule scores
                score = 0.0
                src = f.get("source", "")
                if src == "fingerprint":
                    score += 0.4
                if "apache" in safe_repr(f.get("evidence", "")).lower():
                    score += 0.15
                if "openssh" in safe_repr(f.get("evidence", "")).lower():
                    score += 0.2
                if f.get("confidence", 0) > 0.4:
                    score += 0.1
                prob = min(0.99, score)
                explanation["heuristic"] = True
            risk = self.prob_to_risk(prob)
            results.append({"finding_id": f.get("id"), "probability": prob, "risk": risk, "explanation": explanation})
        return results

    def prob_to_risk(self, prob: float) -> str:
        if prob >= 0.95:
            return "CRITICAL"
        if prob >= RISK_THRESHOLDS["high"]:
            return "HIGH"
        if prob >= RISK_THRESHOLDS["medium"]:
            return "MEDIUM"
        if prob >= RISK_THRESHOLDS["low"]:
            return "LOW"
        return "INFO"

    def train_dummy_model(self, force: bool = False) -> None:
        """
        Train a toy RandomForest model on synthetic data to demonstrate the pipeline.
        The trained model is saved to MODEL_FILE.
        """
        if RandomForestClassifier is None or joblib is None or np is None:
            LOG.warning("ML libraries not installed; skipping model training")
            return

        # If model already exists and not forcing, skip
        if os.path.exists(self.model_file) and not force:
            LOG.info("Model file exists at %s; set force=True to retrain", self.model_file)
            return

        LOG.info("Generating synthetic dataset for demo training")
        # Create synthetic data (1000 samples)
        rows = []
        labels = []
        for i in range(1000):
            # random features: src_code, length, scanner_conf, tls_flag, keywords...
            src_code = np.random.choice([1,2,3,4,5,6])
            length = np.random.randint(0, 2000)
            scanner_conf = np.random.random()
            tls_flag = np.random.choice([0,1])
            kw = np.random.binomial(1, 0.05, 6)  # rare keywords
            # synthetic rule: if fingerprint-like or openssh or apache, more likely vulnerable
            score = 0.0
            if src_code == 6: score += 0.3
            if kw[0] == 1: score += 0.2  # apache
            if kw[1] == 1: score += 0.25  # openssh
            if length > 1000: score += 0.05
            score += scanner_conf * 0.2
            label = 1 if score > 0.35 else 0
            rows.append([src_code, length, scanner_conf, tls_flag] + kw.tolist())
            labels.append(label)

        X = np.array(rows, dtype=float)
        y = np.array(labels, dtype=int)
        LOG.info("Synthetic dataset shapes X=%s y=%s", X.shape, y.shape)
        # train-test split
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        LOG.info("Training RandomForest...")
        model.fit(X_train_scaled, y_train)
        LOG.info("Training complete. Evaluating...")
        X_test_scaled = scaler.transform(X_test)
        preds = model.predict(X_test_scaled)
        report = classification_report(y_test, preds, output_dict=True)
        LOG.info("Classification report: %s", json.dumps(report, indent=2))
        # save model+scaler
        obj = {"model": model, "scaler": scaler}
        joblib.dump(obj, self.model_file)
        self.model = model
        self.scaler = scaler
        LOG.info("Model saved to %s", self.model_file)

# --------------------------
# Reporting utilities
# --------------------------

HTML_TEMPLATE = """
<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>SVSA Report - {scan_id}</title>
<style>
body {{font-family: Arial, sans-serif; margin: 20px; background: #f9f9fb; color: #111;}}
.header {{border-bottom: 1px solid #ddd; padding-bottom: 10px; margin-bottom: 20px;}}
.section {{background: #fff; padding: 12px; border-radius: 6px; box-shadow: 0 1px 2px rgba(0,0,0,0.04); margin-bottom: 12px;}}
.finding {{border-left: 4px solid #3a7; padding-left: 10px; margin-bottom: 8px;}}
.badge {{display:inline-block; padding: 4px 8px; border-radius: 4px; background: #eee; margin-right: 6px; font-size: 12px;}}
.risk-CRITICAL {{background:#9b1c1c;color:#fff;padding:4px 8px;border-radius:4px}}
.risk-HIGH {{background:#d9534f;color:#fff;padding:4px 8px;border-radius:4px}}
.risk-MEDIUM {{background:#f0ad4e;color:#fff;padding:4px 8px;border-radius:4px}}
.risk-LOW {{background:#5bc0de;color:#fff;padding:4px 8px;border-radius:4px}}
.risk-INFO {{background:#6c757d;color:#fff;padding:4px 8px;border-radius:4px}}
</style>
</head>
<body>
<div class="header">
<h1>SVSA Report</h1>
<p>Scan ID: {scan_id} &nbsp; Target: {target} &nbsp; Profile: {profile} &nbsp; Time: {created_at}</p>
</div>
<div class="section">
<h2>Summary</h2>
<p>Total findings: {total_findings}</p>
</div>
<div class="section">
<h2>Findings</h2>
{finding_blocks}
</div>
<div class="section">
<h2>Notes</h2>
<p>Generated by SVSA Monolith. This report contains candidate findings and probabilistic analysis only.</p>
</div>
</body>
</html>
"""

def render_html_report(scan_result: Dict[str, Any], analyses: List[Dict[str, Any]]) -> str:
    finding_blocks = []
    # Map analyses by finding_id
    a_map = {a["finding_id"]: a for a in analyses}
    for f in scan_result.get("findings", []):
        fid = f.get("id")
        name = f.get("name", "Finding")
        target = f.get("target", "")
        desc = f.get("description", "")
        evidence = f.get("evidence", {})
        a = a_map.get(fid)
        risk = a.get("risk", "INFO") if a else "INFO"
        score = a.get("probability", 0.0) if a else 0.0
        cls = f"risk-{risk}"
        block = f"""
        <div class="finding">
            <div><span class="badge">{f.get('source')}</span> <strong>{name}</strong> on <em>{target}</em></div>
            <div style="margin-top:6px"><strong>Risk:</strong> <span class="{cls}">{risk}</span> &nbsp; <strong>Score:</strong> {score:.2f}</div>
            <div style="margin-top:8px"><strong>Description:</strong> {desc}</div>
            <div style="margin-top:8px"><strong>Evidence:</strong> <pre style="white-space:pre-wrap">{json.dumps(evidence, indent=2)}</pre></div>
        </div>
        """
        finding_blocks.append(block)
    html = HTML_TEMPLATE.format(
        scan_id=scan_result.get("id"),
        target=scan_result.get("target"),
        profile=scan_result.get("profile"),
        created_at=scan_result.get("created_at"),
        total_findings=len(scan_result.get("findings", [])),
        finding_blocks="\n".join(finding_blocks),
    )
    return html

# --------------------------
# CLI Entrypoint
# --------------------------

def cli_main(argv: Optional[List[str]] = None) -> int:
    parser = argparse.ArgumentParser(prog="svsa_monolith", description="SVSA Monolith - Scanner & Analyzer")
    sub = parser.add_subparsers(dest="command")

    scan_p = sub.add_parser("scan", help="Run a scan against a target")
    scan_p.add_argument("--target", required=True, help="Target URL or host (e.g., http://example.com or 192.168.1.1)")
    scan_p.add_argument("--profile", choices=list(PROFILES.keys()), default="safe", help="Scanning profile")
    scan_p.add_argument("--confirm-consent", action="store_true", help="Confirm you have authorization to scan the target")
    scan_p.add_argument("--output-json", help="Write JSON summary to file")
    scan_p.add_argument("--output-html", help="Write HTML report to file")

    server_p = sub.add_parser("server", help="Run the HTTP API server")
    server_p.add_argument("--host", default="127.0.0.1")
    server_p.add_argument("--port", default=8000, type=int)

    report_p = sub.add_parser("report", help="Render a report from a saved scan")
    report_p.add_argument("--scan-id", required=True)
    report_p.add_argument("--out", help="Output HTML file", default=None)

    list_p = sub.add_parser("list-results", help="List recent scans")
    list_p.add_argument("--limit", type=int, default=50)

    label_p = sub.add_parser("label", help="Label a finding as true/false")
    label_p.add_argument("--scan-id", required=True)
    label_p.add_argument("--finding-id", required=True)
    label_p.add_argument("--label", choices=["true", "false"], required=True)

    train_p = sub.add_parser("train", help="Train or retrain the ML model (demo synthetic)")
    train_p.add_argument("--force", action="store_true", help="Force retrain even if model exists")

    args = parser.parse_args(argv)

    storage = Storage()
    analyzer = Analyzer()

    if args.command == "scan":
        try:
            require_consent_cli(args.confirm_consent)
        except PermissionError as e:
            LOG.error("Consent required: %s", e)
            return 1
        scanner = Scanner()
        # Run scan
        result = scanner.scan_target(args.target, profile=args.profile, safe=(args.profile == "safe"))
        scan_id = storage.save_scan(args.target, args.profile, result)
        # Save findings
        for f in result.get("findings", []):
            storage.save_finding(scan_id, f.get("source", "unknown"), f.get("target", ""), f.get("evidence", {}))
        # Analyze findings
        analyses = analyzer.predict(result.get("findings", []))
        for a in analyses:
            # find the finding dict to get target
            storage.save_analysis(scan_id, a["finding_id"], a["risk"], a["probability"], a.get("explanation", {}))
        # Render reports if requested
        if args.output_json:
            with open(args.output_json, "w") as fh:
                fh.write(json.dumps({"scan": result, "analyses": analyses}, indent=2))
            LOG.info("Wrote JSON report to %s", args.output_json)
        if args.output_html:
            html = render_html_report(result, analyses)
            with open(args.output_html, "w", encoding="utf-8") as fh:
                fh.write(html)
            LOG.info("Wrote HTML report to %s", args.output_html)
        # Print summary
        print("Scan ID:", scan_id)
        print("Findings:", len(result.get("findings", [])))
        return 0

    elif args.command == "server":
        # Ensure server started with env consent toggle to allow API scans
        require_consent_server()
        if FastAPI is None or uvicorn is None:
            LOG.error("FastAPI/uvicorn not available. Install dependencies to run server.")
            return 1
        app = create_api_app(storage=storage)
        uvicorn.run(app, host=args.host, port=args.port)
        return 0

    elif args.command == "report":
        scan = storage.get_scan(args.scan_id)
        if not scan:
            LOG.error("Scan %s not found", args.scan_id)
            return 1
        # load analyses from DB object
        analyses = scan.get("analyses", [])
        html = render_html_report(scan.get("raw", {"id": scan["id"], "target": scan["target"], "profile": scan["profile"], "created_at": scan["created_at"], "findings": scan.get("findings", [])}), analyses)
        if args.out:
            with open(args.out, "w", encoding="utf-8") as fh:
                fh.write(html)
            LOG.info("Report written to %s", args.out)
        else:
            print(html)
        return 0

    elif args.command == "list-results":
        scans = storage.list_scans(limit=args.limit)
        for s in scans:
            print(f"{s['id']} - {s['target']} - {s['profile']} - {s['created_at']}")
        return 0

    elif args.command == "label":
        lbl = True if args.label == "true" else False
        lid = storage.save_label(args.scan_id, args.finding_id, lbl)
        print("Saved label:", lid)
        return 0

    elif args.command == "train":
        analyzer.train_dummy_model(force=args.force)
        return 0

    else:
        parser.print_help()
        return 1

# --------------------------
# FastAPI HTTP API
# --------------------------

def create_api_app(storage: Storage) -> "FastAPI":
    """
    Construct and return a FastAPI app that wraps scanning and storage.
    Note: API scan endpoints check the SVSA_CONFIRM env var; the server process must be started
    with SVSA_CONFIRM=1 to allow remote scan execution.
    """
    if FastAPI is None:
        raise RuntimeError("FastAPI not installed")

    app = FastAPI(title="SVSA Monolith API", version=VERSION)

    analyzer = Analyzer()
    scanner = Scanner()

    class ScanRequestModel(BaseModel):
        target: str
        profile: str = "safe"

    class LabelRequestModel(BaseModel):
        scan_id: str
        finding_id: str
        label: bool

    @app.on_event("startup")
    def _startup():
        LOG.info("SVSA API starting up. Consent env: %s", os.environ.get(CONFIRM_ENV, "unset"))

    @app.post("/scan")
    async def api_scan(req: ScanRequestModel):
        # Only allow scanning via API if process env contains SVSA_CONFIRM=1
        env_val = os.environ.get(CONFIRM_ENV, "")
        if env_val not in ("1", "true", "True", "yes", "YES"):
            LOG.warning("API scan denied: server not authorized to run scans (missing %s)", CONFIRM_ENV)
            raise HTTPException(status_code=403, detail=f"Server not authorized to run scans. Set {CONFIRM_ENV}=1 to enable.")
        # run scan synchronously (for demo). In production use background tasks/queue.
        try:
            res = scanner.scan_target(req.target, profile=req.profile, safe=(req.profile == "safe"))
            # persist
            sid = storage.save_scan(req.target, req.profile, res)
            for f in res.get("findings", []):
                storage.save_finding(sid, f.get("source", "unknown"), f.get("target", ""), f.get("evidence", {}))
            analyses = analyzer.predict(res.get("findings", []))
            for a in analyses:
                storage.save_analysis(sid, a["finding_id"], a["risk"], a["probability"], a.get("explanation", {}))
            return {"scan_id": sid, "summary": {"findings": len(res.get("findings", []))}}
        except Exception as e:
            LOG.exception("API scan error: %s", e)
            raise HTTPException(status_code=500, detail=str(e))

    @app.get("/results")
    async def api_results(limit: int = 50):
        return storage.list_scans(limit=limit)

    @app.get("/result/{scan_id}")
    async def api_result(scan_id: str):
        s = storage.get_scan(scan_id)
        if not s:
            raise HTTPException(status_code=404, detail="Scan not found")
        return s

    @app.post("/label")
    async def api_label(req: LabelRequestModel):
        # Save label for active learning
        lid = storage.save_label(req.scan_id, req.finding_id, req.label)
        return {"label_id": lid}

    @app.post("/train")
    async def api_train():
        # Dangerous in API; require env consent
        env_val = os.environ.get(CONFIRM_ENV, "")
        if env_val not in ("1", "true", "True", "yes", "YES"):
            raise HTTPException(status_code=403, detail=f"Not authorized to train model via API. Set {CONFIRM_ENV}=1 to enable.")
        analyzer.train_dummy_model(force=True)
        return {"status": "training_started_or_completed"}

    return app

# --------------------------
# Internal test helper (very small)
# --------------------------

def _internal_selftest() -> bool:
    """
    Basic assertions to ensure core functions run in minimal environment.
    This is NOT a substitute for proper unit tests.
    """
    LOG.info("Running selftest")
    # Storage
    st = Storage(":memory:") if False else Storage(DB_FILE + ".selftest")
    sid = st.save_scan("example.com", "safe", {"dummy": True})
    assert sid.startswith("scan_")
    fid = st.save_finding(sid, "tcp_banner", "example.com:22", {"banner": "OpenSSH 7.4"})
    assert fid.startswith("finding_")
    lid = st.save_label(sid, fid, True)
    assert lid.startswith("label_")
    LOG.info("Storage selftest passed")
    # Scanner minimal
    sc = Scanner()
    try:
        r = sc.scan_target("http://example.com", profile="safe", safe=True)
        assert isinstance(r, dict)
    except Exception as e:
        LOG.warning("Scanner selftest encountered error (ok on offline environments): %s", e)
    # Analyzer featurize
    an = Analyzer()
    vec = an.featurize({"source": "tcp_banner", "evidence": {"banner": "OpenSSH 6.0"}, "confidence": 0.3})
    assert isinstance(vec, list)
    LOG.info("Analyzer selftest passed")
    return True

# --------------------------
# If executed as script
# --------------------------
if __name__ == "__main__":
    # Provide a small help header and ensure argument parsing is used
    try:
        ret = cli_main()
        # run internal selftest on explicit env var for debugging
        if os.environ.get("SVSA_SELFTEST", "") in ("1", "true", "True"):
            _internal_selftest()
        sys.exit(ret)
    except KeyboardInterrupt:
        LOG.info("Interrupted by user")
        sys.exit(2)
    except Exception as e:
        LOG.exception("Fatal error: %s", e)
        sys.exit(3)

# End of monolithic file
